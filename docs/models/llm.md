# ðŸ§  LLM Module Documentation

The LLM (Large Language Model) module is the core reasoning backbone of Xoron-Dev. It implements a state-of-the-art Mixture of Experts architecture with several cutting-edge features for efficient long-context processing.

## ðŸ“ File Location

```
models/llm/
â”œâ”€â”€ __init__.py
â””â”€â”€ moe_llama.py    # Main MoE LLaMA implementation
```

## ðŸ—ï¸ Architecture Overview

The LLM is based on a modified LLaMA architecture enhanced with:

1. **Multi-Head Latent Attention (MLA)** - Compressed KV cache
2. **YaRN/LongRoPE** - Superior long-context extrapolation
3. **Ring Attention** - Distributed sequence processing
4. **Aux-Lossless MoE** - Load-balanced expert routing without auxiliary loss

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MoE LLaMA Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input Embeddings (vocab_size=151,643)                         â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Decoder Layer Ã— 12                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  RMSNorm â†’ MLA Attention â†’ Residual             â”‚    â”‚   â”‚
â”‚  â”‚  â”‚     â”‚                                            â”‚    â”‚   â”‚
â”‚  â”‚  â”‚     â–¼                                            â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  RMSNorm â†’ MoE/FFN â†’ Residual                   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  (MoE on layers 1,3,5,7,9,11)                   â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  Final RMSNorm                                                  â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  LM Head (tied with embeddings)                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”‘ Key Components

### 1. YaRN Rotary Embedding

**Purpose**: Enable the model to extrapolate to longer contexts than seen during training.

**How it works**: YaRN (Yet another RoPE extensioN) modifies the standard Rotary Position Embedding by:
- Interpolating frequencies for positions within the original training length
- Extrapolating frequencies for positions beyond the training length
- Using a smooth transition between interpolation and extrapolation

```python
class YaRNRotaryEmbedding(nn.Module):
    """
    YaRN with LongRoPE-style improvements.
    Supports up to 128K+ context with proper frequency scaling.
    """
    def __init__(
        self,
        dim: int,
        max_position_embeddings: int = 131072,  # 128K
        base: float = 500000.0,
        original_max_position_embeddings: int = 8192,
        beta_fast: float = 32.0,
        beta_slow: float = 1.0,
        mscale: float = 1.0,
    ):
        # Compute scaling factor
        self.scaling_factor = max_position_embeddings / original_max_position_embeddings
        
        # YaRN-scaled inverse frequencies
        inv_freq = self._compute_yarn_inv_freq()
```

**Key Parameters**:
- `dim`: Dimension of the embedding (typically head_dim)
- `max_position_embeddings`: Maximum context length (128K)
- `base`: Base frequency (500,000 for long context)
- `beta_fast/beta_slow`: Control the interpolation/extrapolation boundary

**Why it matters**: Standard RoPE degrades significantly beyond training length. YaRN maintains quality at 16x the original context length.

---

### 2. Multi-Head Latent Attention (MLA)

**Purpose**: Reduce KV cache memory by compressing key-value pairs through low-rank projections.

**How it works**: Instead of storing full K and V tensors, MLA:
1. Projects hidden states to a low-rank latent space
2. Stores the compressed representation
3. Expands back to full dimension during attention

```python
class MultiHeadLatentAttention(nn.Module):
    """
    MLA from DeepSeek-V2.
    Compresses KV cache using low-rank projections.
    """
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int = None,
        head_dim: int = None,
        kv_lora_rank: int = 512,  # Compression rank
        q_lora_rank: int = 0,
        ...
    ):
        # KV compression: hidden_size â†’ kv_lora_rank â†’ num_kv_heads * head_dim * 2
        self.kv_a_proj = nn.Linear(hidden_size, kv_lora_rank + head_dim, bias=False)
        self.kv_b_proj = nn.Linear(kv_lora_rank, num_kv_heads * head_dim * 2, bias=False)
        self.kv_a_layernorm = LlamaRMSNorm(kv_lora_rank)
```

**Memory Savings**:
- Standard attention: `2 * batch * seq_len * num_heads * head_dim`
- MLA: `batch * seq_len * kv_lora_rank` (typically 4-8x smaller)

**Why it matters**: Enables longer context windows without running out of GPU memory.

---

### 3. Ring Attention

**Purpose**: Process very long sequences by chunking and accumulating attention in a numerically stable way.

**How it works**: Ring Attention divides the sequence into chunks and processes them iteratively:
1. Compute attention for each chunk
2. Use the log-sum-exp trick to combine results
3. Maintain numerical stability across chunks

```python
def ring_attention(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    chunk_size: int = 4096,
    causal: bool = True,
) -> torch.Tensor:
    """
    Ring Attention for distributed long-context processing.
    
    For sequences longer than chunk_size:
    1. Process in chunks
    2. Accumulate using log-sum-exp for stability
    3. Apply causal masking per chunk
    """
    # For short sequences, use standard attention
    if seq_len <= chunk_size and kv_len <= chunk_size:
        return standard_attention(query, key, value)
    
    # For long sequences, chunk and accumulate
    for kv_idx in range(num_kv_chunks):
        # Compute attention for this chunk
        attn_chunk = torch.matmul(query, key_chunk.transpose(-1, -2)) * scale
        
        # Log-sum-exp accumulation for numerical stability
        chunk_max = attn_chunk.max(dim=-1, keepdim=True)[0]
        new_max = torch.maximum(max_logits, chunk_max)
        
        # Update running sum with correction factor
        correction = torch.exp(max_logits - new_max)
        output = output * correction + torch.matmul(exp_weights, value_chunk)
```

**Why it matters**: Enables 128K+ context without quadratic memory growth.

---

### 4. Aux-Lossless MoE Router

**Purpose**: Route tokens to experts without needing auxiliary load-balancing loss.

**How it works**: Traditional MoE requires auxiliary loss to prevent expert collapse. Aux-Lossless MoE achieves balance through:
1. Input normalization before routing
2. Learnable expert bias terms
3. Normalized top-k probability selection

```python
class AuxLosslessMoERouter(nn.Module):
    """
    Aux-Lossless MoE Router with Shared Expert Isolation.
    Eliminates auxiliary loss while maintaining load balance.
    """
    def __init__(
        self,
        hidden_size: int,
        num_experts: int,
        top_k: int = 2,
        norm_topk_prob: bool = True,
    ):
        self.input_norm = LlamaRMSNorm(hidden_size)
        self.gate = nn.Linear(hidden_size, num_experts, bias=False)
        
    def forward(self, hidden_states):
        # Normalize input for stable routing
        hidden_norm = self.input_norm(hidden_flat)
        router_logits = self.gate(hidden_norm)
        
        # Softmax routing probabilities
        router_probs = F.softmax(router_logits, dim=-1)
        
        # Select top-k experts
        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)
        
        # Normalize selected probabilities
        if self.norm_topk_prob:
            top_k_probs = top_k_probs / (top_k_probs.sum(dim=-1, keepdim=True) + EPS)
```

**Why it matters**: Simpler training (no aux loss tuning), better convergence, and natural load balancing.

---

### 5. MoE Layer with Shared Expert

**Purpose**: Combine specialized experts with a shared expert for both specialization and generalization.

**Architecture**:
```
Input
  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                      â”‚
  â–¼                                      â–¼
Router â†’ Expert 1,2,...,8           Shared Expert
  â”‚      (top-2 selected)                â”‚
  â”‚                                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
            Sum outputs
```

```python
class AuxLosslessMoELayer(nn.Module):
    """MoE Layer with Shared Expert Isolation (DeepSeek-style)."""
    
    def forward(self, hidden_states):
        # Route to top-k experts
        top_k_probs, top_k_indices, _ = self.router(hidden_states)
        
        # Compute expert outputs (weighted sum of selected experts)
        for expert_idx in range(self.num_experts):
            expert = self.experts[expert_idx]
            for k in range(self.top_k):
                mask = (top_k_indices[:, k] == expert_idx)
                if mask.any():
                    expert_output = expert(x_flat[mask])
                    output[mask] += top_k_probs[mask, k:k+1] * expert_output
        
        # Always add shared expert output
        shared_output = self.shared_expert(x_flat)
        output = output + shared_output
```

**Why it matters**: The shared expert captures common patterns while specialized experts handle specific domains.

---

### 6. MoE Expert (SwiGLU FFN)

**Purpose**: Each expert is a feed-forward network with SwiGLU activation.

```python
class MoEExpert(nn.Module):
    """Single expert FFN with SwiGLU activation."""
    
    def __init__(self, hidden_size: int, intermediate_size: int):
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.act_fn = nn.SiLU()
    
    def forward(self, x):
        # SwiGLU: SiLU(gate) * up, then down
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
```

**SwiGLU Formula**: `output = down(SiLU(gate(x)) * up(x))`

**Why SwiGLU**: Better performance than ReLU or GELU, especially for language modeling.

---

## ðŸ“Š Model Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| `hidden_size` | 1024 | Model dimension |
| `num_layers` | 12 | Number of transformer layers |
| `num_heads` | 16 | Attention heads |
| `intermediate_size` | 2048 | FFN intermediate dimension |
| `vocab_size` | 151,643 | Qwen2.5 vocabulary |
| `max_position_embeddings` | 131,072 | 128K context |
| `num_experts` | 8 | Routed experts |
| `num_experts_per_tok` | 2 | Top-k routing |
| `moe_layer_freq` | 2 | MoE every 2nd layer |
| `kv_lora_rank` | 512 | MLA compression rank |
| `ring_chunk_size` | 4096 | Ring attention chunk |

---

## ðŸ”„ Forward Pass Flow

```python
def forward(self, input_ids, attention_mask=None, labels=None, ...):
    # 1. Embed tokens
    hidden_states = self.embed_tokens(input_ids)
    
    # 2. Process through decoder layers
    for layer in self.layers:
        # Self-attention with MLA
        hidden_states = layer.input_layernorm(hidden_states)
        attn_output = layer.self_attn(hidden_states, ...)
        hidden_states = residual + attn_output
        
        # FFN or MoE
        hidden_states = layer.post_attention_layernorm(hidden_states)
        if layer.is_moe_layer:
            hidden_states, aux_loss = layer.mlp(hidden_states)
        else:
            hidden_states = layer.mlp(hidden_states)
        hidden_states = residual + hidden_states
    
    # 3. Final normalization
    hidden_states = self.norm(hidden_states)
    
    # 4. Language model head
    logits = self.lm_head(hidden_states)
    
    # 5. Compute loss if labels provided
    if labels is not None:
        loss = cross_entropy(logits, labels)
```

---

## ðŸŽ¯ Generation

The model supports autoregressive generation with KV caching:

```python
@torch.no_grad()
def generate(
    self,
    input_ids: torch.Tensor,
    max_new_tokens: int = 100,
    temperature: float = 1.0,
    top_k: int = 50,
    top_p: float = 0.9,
    do_sample: bool = True,
):
    past_key_values = None
    
    for _ in range(max_new_tokens):
        # Prepare inputs (only last token if using cache)
        model_inputs = self.prepare_inputs_for_generation(
            input_ids, past_key_values=past_key_values
        )
        
        # Forward pass with caching
        outputs = self.forward(**model_inputs, use_cache=True)
        
        # Sample next token
        next_token_logits = outputs.logits[:, -1, :] / temperature
        
        if do_sample:
            # Top-k filtering
            if top_k > 0:
                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                next_token_logits[indices_to_remove] = float('-inf')
            
            # Top-p (nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                # Remove tokens with cumulative probability above threshold
                ...
            
            # Sample from distribution
            probs = F.softmax(next_token_logits, dim=-1)
            next_tokens = torch.multinomial(probs, num_samples=1)
        else:
            # Greedy decoding
            next_tokens = torch.argmax(next_token_logits, dim=-1)
        
        # Update sequence and cache
        input_ids = torch.cat([input_ids, next_tokens.unsqueeze(-1)], dim=-1)
        past_key_values = outputs.past_key_values
        
        # Check for EOS
        if (next_tokens == eos_token_id).all():
            break
    
    return input_ids
```

---

## ðŸ’¡ Design Decisions

### Why MLA over Standard Attention?
- **Memory**: 4-8x reduction in KV cache size
- **Speed**: Faster inference with smaller cache
- **Quality**: Minimal accuracy loss with proper rank selection

### Why Aux-Lossless MoE?
- **Simplicity**: No hyperparameter tuning for aux loss weight
- **Stability**: More stable training dynamics
- **Performance**: Comparable or better than aux-loss variants

### Why Ring Attention?
- **Scalability**: Linear memory growth with sequence length
- **Flexibility**: Works with any chunk size
- **Compatibility**: Integrates with Flash Attention

### Why Shared Expert?
- **Generalization**: Captures common patterns across all inputs
- **Specialization**: Routed experts handle domain-specific knowledge
- **Efficiency**: Better parameter utilization

---

## ðŸ”§ Usage Example

```python
from models.llm.moe_llama import MoELlamaForCausalLM
from transformers import LlamaConfig

# Create config
config = LlamaConfig(
    vocab_size=151643,
    hidden_size=1024,
    intermediate_size=2048,
    num_hidden_layers=12,
    num_attention_heads=16,
    max_position_embeddings=131072,
)

# MoE configuration
moe_config = {
    'use_moe': True,
    'num_experts': 8,
    'num_experts_per_tok': 2,
    'moe_layer_freq': 2,
    'intermediate_size': 2048,
}

# Initialize model
model = MoELlamaForCausalLM(config, moe_config)

# Forward pass
outputs = model(
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels,
)

# Access outputs
loss = outputs.loss
logits = outputs.logits
aux_loss = outputs.aux_loss  # MoE auxiliary loss (for monitoring)
```

---

## ðŸ“ˆ Performance Characteristics

| Metric | Value |
|--------|-------|
| Parameters (active) | ~1.2B |
| Parameters (total) | ~2.8B |
| Context Length | 128K tokens |
| Inference Speed | ~50 tokens/sec (A100) |
| Memory (inference) | ~8GB (FP16) |
| Memory (training) | ~24GB (FP16 + gradients) |

---

## ðŸ”— Related Documentation

- [Components Documentation](components.md) - Attention, LoRA, MoE details
- [Training Documentation](../training/README.md) - How to train the LLM
- [Config Documentation](../config/README.md) - Configuration options
